<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ruwwww.github.io/al-folio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ruwwww.github.io/al-folio/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-03-01T15:41:58+00:00</updated><id>https://ruwwww.github.io/al-folio/feed.xml</id><title type="html">ruwwww</title><subtitle>Personal academic website of Abdurrahman Izzuddin Al Faruq, undergraduate student at Institut Teknologi Sepuluh November. </subtitle><entry><title type="html">Training a Waifu Diffusion Model with Patch Diffusion and Rectified Flow</title><link href="https://ruwwww.github.io/al-folio/blog/2026/waifu-diffusion/" rel="alternate" type="text/html" title="Training a Waifu Diffusion Model with Patch Diffusion and Rectified Flow"/><published>2026-03-01T00:00:00+00:00</published><updated>2026-03-01T00:00:00+00:00</updated><id>https://ruwwww.github.io/al-folio/blog/2026/waifu-diffusion</id><content type="html" xml:base="https://ruwwww.github.io/al-folio/blog/2026/waifu-diffusion/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Training generative models on small, imbalanced datasets is notoriously difficult—but it’s doable with the right tricks. I trained a <strong>130M-parameter diffusion model</strong> on just 10,000 anime faces, 90% of them <strong>monochrome</strong>, that still generates coherent colorful images. This post covers the key techniques: <strong>patch diffusion</strong>, <strong>rectified flow</strong>, and <strong>CIELAB color space decoupling</strong>.</p> <p><strong>Weights &amp; Code</strong>: <a href="https://huggingface.co/ruwwww/waifu_diffusion">ruwwww/waifu_diffusion</a></p> <hr/> <h2 id="part-1-diffusion--flow-matching-primer">Part 1: Diffusion &amp; Flow Matching Primer</h2> <h3 id="diffusion-models-the-basics">Diffusion Models: The Basics</h3> <p>Diffusion models learn to reverse a noise corruption process. You start with data $x_1$ and gradually add noise:</p> \[x_t = \sqrt{\bar{\alpha}_t} x_1 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\] <p>The model learns to predict the noise $\epsilon_\theta(x_t, t)$ at each step, allowing deterministic sampling through <strong>many denoising steps</strong> (often 50–1000).</p> <h3 id="rectified-flow-straight-line-paths">Rectified Flow: Straight-Line Paths</h3> <p><strong>Rectified flow</strong> simplifies this by learning a velocity field along a <em>straight path</em> from noise to data:</p> \[x_t = (1-t) x_0 + t x_1, \quad t \in [0, 1]\] <p>Instead of predicting noise, the model learns velocity: \(v_\theta(x_t, t)\)</p> <p><strong>Why it’s better</strong>: Straight paths require fewer steps (30–50 work well), and the linear time mapping is more natural for learning.</p> <p>In practice, we predict the <strong>clean image</strong> $\hat{x}_1$ and derive velocity: \(v = \frac{\hat{x}_1 - x_t}{1 - t}\)</p> <hr/> <h2 id="part-2-handling-imbalanced-data-with-cielab">Part 2: Handling Imbalanced Data with CIELAB</h2> <h3 id="the-dataset-problem">The Dataset Problem</h3> <p>We use <a href="https://huggingface.co/datasets/amirali900/Anime-Face-Dataset-10k">Anime-Face-Dataset-10k</a>:</p> <ul> <li>10,000 native 80×80 images</li> <li><strong>90% corrupted to monochrome</strong></li> <li>10% kept in color</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/waifu-diffusion/monochrome-corruption-480.webp 480w,/al-folio/assets/img/posts/waifu-diffusion/monochrome-corruption-800.webp 800w,/al-folio/assets/img/posts/waifu-diffusion/monochrome-corruption-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/waifu-diffusion/monochrome-corruption.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Monochrome corruption examples: 4 pairs showing original color images (left) and their transformed grayscale versions (right) </div> <h3 id="why-cielab-works-better-than-rgb">Why CIELAB Works Better Than RGB</h3> <p>Rather than train in RGB, we convert to <strong>CIE L*a*b*</strong> color space:</p> \[L \in [0, 100] \quad \text{(luminance)}, \quad a \in [-128, 128] \quad \text{(green-red)}, \quad b \in [-128, 128] \quad \text{(blue-yellow)}\] <p><strong>Key insight</strong>: L*a*b* decouples structure (L) from color (a, b). For monochrome images, we zero out the chroma channels and mask gradients:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># For monochrome images
</span><span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">color_indices</span><span class="p">:</span>
    <span class="n">lab_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>     <span class="c1"># Zero chroma
</span>    <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>            <span class="c1"># No gradient flow
</span></code></pre></div></div> <p>This lets the model learn structural features from all 10k samples while learning color specifically from the 1k color samples without interference.</p> <hr/> <h2 id="part-3-patch-diffusion-for-data-augmentation">Part 3: Patch Diffusion for Data Augmentation</h2> <h3 id="the-strategy">The Strategy</h3> <p>With only 10k samples, training on full 80×80 images is risky. We use <strong>random patch cropping</strong> during training:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">patch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">80</span><span class="p">]</span>  <span class="c1"># Variable size patches
</span><span class="n">full_image_prob</span> <span class="o">=</span> <span class="mf">0.20</span>

<span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">full_image_prob</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_full</span>  <span class="c1"># Full image
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">patch_sizes</span><span class="p">)</span>
    <span class="n">top</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">80</span> <span class="o">-</span> <span class="n">size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="n">left</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">80</span> <span class="o">-</span> <span class="n">size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_full</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">top</span><span class="o">*</span><span class="mi">4</span><span class="p">:</span><span class="n">top</span><span class="o">*</span><span class="mi">4</span><span class="o">+</span><span class="n">size</span><span class="p">,</span> <span class="n">left</span><span class="o">*</span><span class="mi">4</span><span class="p">:</span><span class="n">left</span><span class="o">*</span><span class="mi">4</span><span class="o">+</span><span class="n">size</span><span class="p">]</span>
</code></pre></div></div> <p>A 40×40 patch can appear at up to 21 positions, effectively multiplying dataset size. We use <strong>Vision Rotary Embeddings</strong> to ensure spatial consistency across patches.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/waifu-diffusion/patch-diagram-480.webp 480w,/al-folio/assets/img/posts/waifu-diffusion/patch-diagram-800.webp 800w,/al-folio/assets/img/posts/waifu-diffusion/patch-diagram-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/waifu-diffusion/patch-diagram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Patch diffusion strategy: A 40×40 patch can be cropped from multiple positions in the 80×80 image, effectively augmenting the dataset </div> <hr/> <h2 id="part-4-model-architecture">Part 4: Model Architecture</h2> <h3 id="the-jit-transformer">The JiT Transformer</h3> <p>We use a modern <strong>Diffusion Transformer (DiT-B)</strong> with ~130M parameters:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">JiT</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>           <span class="c1"># 80×80 → 20×20 token grid
</span>    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">in_context_len</span><span class="o">=</span><span class="mi">0</span>        <span class="c1"># Unconditional
</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Key modern techniques</strong>:</p> <ul> <li><strong>AdaLN</strong>: Timestep-modulated layer norm for expressive conditioning</li> <li><strong>SwiGLU</strong>: Swish-gated feedforward layers (better than standard MLPs)</li> <li><strong>RMSNorm</strong>: Stable layer normalization for mixed-precision training</li> <li><strong>Vision RoPE</strong>: 2D rotary positional embeddings for patch-aware spatial reasoning</li> <li><strong>Scaled Dot-Product Attention</strong>: Memory-efficient attention</li> </ul> <p>The attention mechanism applies Vision RoPE with patch coordinates, ensuring smooth spatial transitions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span> <span class="o">=</span> <span class="nf">rope</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="n">top_idx</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="n">left_idx</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="nf">rope</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="n">top_idx</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="n">left_idx</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="part-5-training-strategy">Part 5: Training Strategy</h2> <h3 id="loss-function--gradient-masking">Loss Function &amp; Gradient Masking</h3> <p>We train the velocity field with <strong>masked MSE loss</strong>. The model outputs <strong>x-pred</strong> (clean image prediction), which we convert to <strong>v-pred</strong> for both loss computation and sampling:</p> \[\mathcal{L} = \mathbb{E} \left[ \| v_\theta(x_t, t) - (x_1 - x_0) \|^2 \odot \mathbf{m} \right]\] <p>where $v_\theta = \hat{x}_1 - x_t$ is derived from the model’s x-prediction.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">masked_mse_loss</span><span class="p">(</span><span class="n">pred_x1</span><span class="p">,</span> <span class="n">target_x1</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">pred_x1</span><span class="p">,</span> <span class="n">target_x1</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

<span class="c1"># Training loop converts x-pred to v-pred:
</span><span class="n">v_pred</span> <span class="o">=</span> <span class="n">pred_x1</span> <span class="o">-</span> <span class="n">x0</span>
<span class="n">v_target</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">v_pred</span> <span class="o">-</span> <span class="n">v_target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">mask</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p>The mask prevents gradients from flowing through masked chroma channels in monochrome images.</p> <h3 id="handling-data-imbalance-oversampling">Handling Data Imbalance: Oversampling</h3> <p>We oversample colored images by 3x using <code class="language-plaintext highlighter-rouge">WeightedRandomSampler</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">color_indices</span> <span class="k">else</span> <span class="mf">1.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))]</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="nc">WeightedRandomSampler</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">replacement</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="training-details">Training Details</h3> <ul> <li><strong>Epochs</strong>: 1280</li> <li><strong>Batch size</strong>: 256 (accumulated from 4×64)</li> <li><strong>Learning rate</strong>: 3e-4 (AdamW)</li> <li><strong>Mixed precision</strong>: fp16 with gradient scaling</li> <li><strong>Compilation</strong>: <code class="language-plaintext highlighter-rouge">torch.compile</code> for 1.5–2x speedup</li> </ul> <hr/> <h2 id="part-6-results">Part 6: Results</h2> <h3 id="generation-sampling">Generation Sampling</h3> <p>Sampling uses <strong>Euler integration</strong> over 50 steps:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">t_val</span> <span class="o">=</span> <span class="n">step</span> <span class="o">/</span> <span class="n">steps</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">t_val</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">pred_x1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_x1</span> <span class="o">-</span> <span class="n">xt</span><span class="p">)</span> <span class="o">/</span> <span class="nf">max</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">t_val</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
        <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">+</span> <span class="n">v</span> <span class="o">/</span> <span class="n">steps</span>

    <span class="k">return</span> <span class="n">pred_x1</span>
</code></pre></div></div> <h3 id="generated-images">Generated Images</h3> <p>Despite training on 90% monochrome data, the model generates vibrant, coherent faces:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/waifu-diffusion/generated-samples-480.webp 480w,/al-folio/assets/img/posts/waifu-diffusion/generated-samples-800.webp 800w,/al-folio/assets/img/posts/waifu-diffusion/generated-samples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/waifu-diffusion/generated-samples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Three generated anime face samples from the model, all at native 80×80 resolution </div> <h3 id="avoiding-memorization-lpips-validation">Avoiding Memorization: LPIPS Validation</h3> <p>We verify each generated image is novel by computing <strong>LPIPS distance</strong> to the nearest training sample. Typical values:</p> <ul> <li>Same image: 0.0</li> <li>Very similar: 0.05–0.15</li> <li>Somewhat distinct: 0.15–0.25</li> <li>Our generated samples: <strong>≥ 0.25–0.3</strong> ✓</li> </ul> <p>The consistently higher LPIPS scores confirm the model generates novel faces rather than memorizing training data.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/waifu-diffusion/nearest-neighbor-480.webp 480w,/al-folio/assets/img/posts/waifu-diffusion/nearest-neighbor-800.webp 800w,/al-folio/assets/img/posts/waifu-diffusion/nearest-neighbor-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/waifu-diffusion/nearest-neighbor.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Nearest neighbor validation (merged panel): each pair shows a generated image and its closest training sample with LPIPS distance ≥ 0.25 </div> <h3 id="generation-trajectory">Generation Trajectory</h3> <p>The model smoothly transitions from noise to structure to detail:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/waifu-diffusion/trajectory-480.webp 480w,/al-folio/assets/img/posts/waifu-diffusion/trajectory-800.webp 800w,/al-folio/assets/img/posts/waifu-diffusion/trajectory-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/waifu-diffusion/trajectory.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Generation trajectory showing 10 frames from pure noise (t=0) to final image (t=1) over 50 sampling steps </div> <hr/> <h2 id="part-7-key-takeaways">Part 7: Key Takeaways</h2> <ol> <li> <p><strong>CIELAB decoupling is powerful</strong>: Separating structure from color lets you learn from partial/corrupted data gracefully.</p> </li> <li> <p><strong>Patch diffusion multiplies your data</strong>: Random cropping with spatial embeddings is a simple but effective augmentation.</p> </li> <li> <p><strong>Rectified flow is simpler &amp; faster</strong>: Straight-line paths with velocity matching need far fewer steps than traditional DDPM.</p> </li> <li> <p><strong>Modern components matter</strong>: AdaLN, RMSNorm, Vision RoPE, and torch.compile all contribute to efficiency.</p> </li> <li> <p><strong>Oversampling works</strong>: Weighted sampling of rare color samples prevents the model from ignoring them.</p> </li> </ol> <hr/> <h2 id="part-8-code--model">Part 8: Code &amp; Model</h2> <p><strong>Model weights</strong>: <a href="https://huggingface.co/ruwwww/waifu_diffusion">ruwwww/waifu_diffusion</a><br/> <strong>File</strong>: <code class="language-plaintext highlighter-rouge">waifu_diffusion_1280_bs256.safetensors</code> (130M params)</p> <p><strong>Quick inference</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">safetensors.torch</span> <span class="kn">import</span> <span class="n">load_file</span>
<span class="kn">from</span> <span class="n">skimage</span> <span class="kn">import</span> <span class="n">color</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">JiT</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="nf">load_file</span><span class="p">(</span><span class="sh">"</span><span class="s">waifu_diffusion_1280_bs256.safetensors</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># Generate
</span><span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">step</span> <span class="o">/</span> <span class="mi">50</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">pred_x1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_x1</span> <span class="o">-</span> <span class="n">xt</span><span class="p">)</span> <span class="o">/</span> <span class="nf">max</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">step</span><span class="o">/</span><span class="mi">50</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">+</span> <span class="n">v</span> <span class="o">/</span> <span class="mi">50</span>

<span class="c1"># Convert CIELAB → RGB
</span><span class="n">lab</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">pred_x1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="n">lab</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">50</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">lab</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">lab</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">rgb</span> <span class="o">=</span> <span class="n">color</span><span class="p">.</span><span class="nf">lab2rgb</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">L</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div> <p>Full training code is in the repository.</p> <hr/> <h2 id="references">References</h2> <ul> <li><strong>Rectified Flow</strong>: <a href="https://arxiv.org/abs/2210.02747">Flow Matching for Generative Modeling</a></li> <li><strong>DiT</strong>: <a href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a></li> <li><strong>Vision RoPE</strong>: <a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></li> </ul> <hr/> <table> <tbody> <tr> <td><em>March 2026</em></td> <td>Model: <a href="https://huggingface.co/ruwwww/waifu_diffusion">ruwwww/waifu_diffusion</a></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="ML-Engineering"/><category term="diffusion-models"/><category term="generative-ai"/><category term="flow-matching"/><category term="efficient-learning"/><summary type="html"><![CDATA[How to train a data-efficient diffusion model on corrupted anime face data using CIELAB space, patch cropping, and modern transformer techniques.]]></summary></entry></feed>