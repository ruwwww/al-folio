<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Training a Waifu Diffusion Model with Patch Diffusion and Rectified Flow | ruwwww </title> <meta name="author" content="Abdurrahman Izzuddin Al Faruq"> <meta name="description" content="How to train a data-efficient diffusion model on corrupted anime face data using CIELAB space, patch cropping, and modern transformer techniques."> <meta name="keywords" content="software engineering, machine learning, backend, mlops, generative models, diffusion models"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/al-folio/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/al-folio/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/al-folio/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/al-folio/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruwwww.github.io/al-folio/blog/2026/waifu-diffusion/"> <script src="/al-folio/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/al-folio/"> ruwwww </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/al-folio/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/al-folio/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/cv/">CV </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/al-folio/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/al-folio/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Training a Waifu Diffusion Model with Patch Diffusion and Rectified Flow</h1> <p class="post-meta"> Created on March 01, 2026 </p> <p class="post-tags"> <a href="/al-folio/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/al-folio/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> diffusion-models</a>   <a href="/al-folio/blog/tag/generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> generative-ai</a>   <a href="/al-folio/blog/tag/flow-matching"> <i class="fa-solid fa-hashtag fa-sm"></i> flow-matching</a>   <a href="/al-folio/blog/tag/efficient-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> efficient-learning</a>   ·   <a href="/al-folio/blog/category/ml-engineering"> <i class="fa-solid fa-tag fa-sm"></i> ML-Engineering</a> </p> </header> <article class="post-content"><div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>Training generative models on small, imbalanced datasets is notoriously difficult—but it’s doable with the right tricks. I trained a <strong>130M-parameter diffusion model</strong> on just 10,000 anime faces, 90% of them <strong>monochrome</strong>, that still generates coherent colorful images. This post covers the key techniques: <strong>patch diffusion</strong>, <strong>rectified flow</strong>, and <strong>CIELAB color space decoupling</strong>.</p> <p><strong>Weights &amp; Code</strong>: <a href="https://huggingface.co/ruwwww/waifu_diffusion" rel="external nofollow noopener" target="_blank">ruwwww/waifu_diffusion</a></p> <hr> <h2 id="part-1-diffusion--flow-matching-primer">Part 1: Diffusion &amp; Flow Matching Primer</h2> <h3 id="diffusion-models-the-basics">Diffusion Models: The Basics</h3> <p>Diffusion models learn to reverse a noise corruption process. You start with data $x_1$ and gradually add noise:</p> \[x_t = \sqrt{\bar{\alpha}_t} x_1 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\] <p>The model learns to predict the noise $\epsilon_\theta(x_t, t)$ at each step, allowing deterministic sampling through <strong>many denoising steps</strong> (often 50–1000).</p> <h3 id="rectified-flow-straight-line-paths">Rectified Flow: Straight-Line Paths</h3> <p><strong>Rectified flow</strong> simplifies this by learning a velocity field along a <em>straight path</em> from noise to data:</p> \[x_t = (1-t) x_0 + t x_1, \quad t \in [0, 1]\] <p>Instead of predicting noise, the model learns velocity: \(v_\theta(x_t, t)\)</p> <p><strong>Why it’s better</strong>: Straight paths require fewer steps (30–50 work well), and the linear time mapping is more natural for learning.</p> <p>In practice, we predict the <strong>clean image</strong> $\hat{x}_1$ and derive velocity: \(v = \frac{\hat{x}_1 - x_t}{1 - t}\)</p> <hr> <h2 id="part-2-handling-imbalanced-data-with-cielab">Part 2: Handling Imbalanced Data with CIELAB</h2> <h3 id="the-dataset-problem">The Dataset Problem</h3> <p>We use <a href="https://huggingface.co/datasets/amirali900/Anime-Face-Dataset-10k" rel="external nofollow noopener" target="_blank">Anime-Face-Dataset-10k</a>:</p> <ul> <li>10,000 native 80×80 images</li> <li><strong>90% corrupted to monochrome</strong></li> <li>10% kept in color</li> </ul> <p><strong>[Figure: 4 pairs showing (color original, transformed grayscale) examples]</strong></p> <h3 id="why-cielab-works-better-than-rgb">Why CIELAB Works Better Than RGB</h3> <p>Rather than train in RGB, we convert to <strong>CIE L*a*b*</strong> color space:</p> \[L \in [0, 100] \quad \text{(luminance)}, \quad a \in [-128, 128] \quad \text{(green-red)}, \quad b \in [-128, 128] \quad \text{(blue-yellow)}\] <p><strong>Key insight</strong>: L*a*b* decouples structure (L) from color (a, b). For monochrome images, we zero out the chroma channels and mask gradients:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># For monochrome images
</span><span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">color_indices</span><span class="p">:</span>
    <span class="n">lab_tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>     <span class="c1"># Zero chroma
</span>    <span class="n">mask</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>            <span class="c1"># No gradient flow
</span></code></pre></div></div> <p>This lets the model learn structural features from all 10k samples while learning color specifically from the 1k color samples without interference.</p> <hr> <h2 id="part-3-patch-diffusion-for-data-augmentation">Part 3: Patch Diffusion for Data Augmentation</h2> <h3 id="the-strategy">The Strategy</h3> <p>With only 10k samples, training on full 80×80 images is risky. We use <strong>random patch cropping</strong> during training:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">patch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">80</span><span class="p">]</span>  <span class="c1"># Variable size patches
</span><span class="n">full_image_prob</span> <span class="o">=</span> <span class="mf">0.20</span>

<span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">full_image_prob</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_full</span>  <span class="c1"># Full image
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">patch_sizes</span><span class="p">)</span>
    <span class="n">top</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">80</span> <span class="o">-</span> <span class="n">size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="n">left</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">80</span> <span class="o">-</span> <span class="n">size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_full</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">top</span><span class="o">*</span><span class="mi">4</span><span class="p">:</span><span class="n">top</span><span class="o">*</span><span class="mi">4</span><span class="o">+</span><span class="n">size</span><span class="p">,</span> <span class="n">left</span><span class="o">*</span><span class="mi">4</span><span class="p">:</span><span class="n">left</span><span class="o">*</span><span class="mi">4</span><span class="o">+</span><span class="n">size</span><span class="p">]</span>
</code></pre></div></div> <p>A 40×40 patch can appear at up to 21 positions, effectively multiplying dataset size. We use <strong>Vision Rotary Embeddings</strong> to ensure spatial consistency across patches.</p> <p><strong>[Figure: Visualization showing how a 40×40 patch can be sampled from different positions in the full 80×80 image]</strong></p> <hr> <h2 id="part-4-model-architecture">Part 4: Model Architecture</h2> <h3 id="the-jit-transformer">The JiT Transformer</h3> <p>We use a modern <strong>Diffusion Transformer (DiT-B)</strong> with ~130M parameters:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">JiT</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>           <span class="c1"># 80×80 → 20×20 token grid
</span>    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">in_context_len</span><span class="o">=</span><span class="mi">0</span>        <span class="c1"># Unconditional
</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Key modern techniques</strong>:</p> <ul> <li> <strong>AdaLN</strong>: Timestep-modulated layer norm for expressive conditioning</li> <li> <strong>SwiGLU</strong>: Swish-gated feedforward layers (better than standard MLPs)</li> <li> <strong>RMSNorm</strong>: Stable layer normalization for mixed-precision training</li> <li> <strong>Vision RoPE</strong>: 2D rotary positional embeddings for patch-aware spatial reasoning</li> <li> <strong>Scaled Dot-Product Attention</strong>: Memory-efficient attention</li> </ul> <p>The attention mechanism applies Vision RoPE with patch coordinates, ensuring smooth spatial transitions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span> <span class="o">=</span> <span class="nf">rope</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="n">top_idx</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="n">left_idx</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="nf">rope</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="n">top_idx</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="n">left_idx</span><span class="p">)</span>
</code></pre></div></div> <hr> <h2 id="part-5-training-strategy">Part 5: Training Strategy</h2> <h3 id="loss-function--gradient-masking">Loss Function &amp; Gradient Masking</h3> <p>We train the velocity field with <strong>masked MSE loss</strong>:</p> \[\mathcal{L} = \mathbb{E} \left[ \| v_\theta(x_t, t) - (x_1 - x_0) \|^2 \odot \mathbf{m} \right]\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">masked_mse_loss</span><span class="p">(</span><span class="n">pred_x1</span><span class="p">,</span> <span class="n">target_x1</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">pred_x1</span><span class="p">,</span> <span class="n">target_x1</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p>The mask prevents gradients from flowing through masked chroma channels in monochrome images.</p> <h3 id="handling-data-imbalance-oversampling">Handling Data Imbalance: Oversampling</h3> <p>We oversample colored images by 3x using <code class="language-plaintext highlighter-rouge">WeightedRandomSampler</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">color_indices</span> <span class="k">else</span> <span class="mf">1.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))]</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="nc">WeightedRandomSampler</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">replacement</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="training-details">Training Details</h3> <ul> <li> <strong>Epochs</strong>: 1280</li> <li> <strong>Batch size</strong>: 256 (accumulated from 4×64)</li> <li> <strong>Learning rate</strong>: 3e-4 (AdamW)</li> <li> <strong>Mixed precision</strong>: fp16 with gradient scaling</li> <li> <strong>Compilation</strong>: <code class="language-plaintext highlighter-rouge">torch.compile</code> for 1.5–2x speedup</li> </ul> <hr> <h2 id="part-6-results">Part 6: Results</h2> <h3 id="generation-sampling">Generation Sampling</h3> <p>Sampling uses <strong>Euler integration</strong> over 50 steps:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">t_val</span> <span class="o">=</span> <span class="n">step</span> <span class="o">/</span> <span class="n">steps</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">t_val</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">pred_x1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_x1</span> <span class="o">-</span> <span class="n">xt</span><span class="p">)</span> <span class="o">/</span> <span class="nf">max</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">t_val</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
        <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">+</span> <span class="n">v</span> <span class="o">/</span> <span class="n">steps</span>

    <span class="k">return</span> <span class="n">pred_x1</span>
</code></pre></div></div> <h3 id="generated-images">Generated Images</h3> <p>Despite training on 90% monochrome data, the model generates vibrant, coherent faces:</p> <p><strong>[Figure: 3 generated anime face samples]</strong></p> <h3 id="avoiding-memorization-lpips-validation">Avoiding Memorization: LPIPS Validation</h3> <p>We verify each generated image is novel by computing <strong>LPIPS distance</strong> to the nearest training sample. Typical values:</p> <ul> <li>Same image: 0.0</li> <li>Very similar: 0.1–0.3</li> <li>Distinct images: 1.0–2.0</li> <li>Our generated samples: <strong>&gt; 3.0</strong> ✓</li> </ul> <p><strong>[Figure: 3 pairs of (generated sample, nearest training neighbor) with LPIPS scores]</strong></p> <h3 id="generation-trajectory">Generation Trajectory</h3> <p>The model smoothly transitions from noise to structure to detail:</p> <p><strong>[Figure: 1 trajectory showing 10 frames from noise → final image]</strong></p> <hr> <h2 id="part-7-key-takeaways">Part 7: Key Takeaways</h2> <ol> <li> <p><strong>CIELAB decoupling is powerful</strong>: Separating structure from color lets you learn from partial/corrupted data gracefully.</p> </li> <li> <p><strong>Patch diffusion multiplies your data</strong>: Random cropping with spatial embeddings is a simple but effective augmentation.</p> </li> <li> <p><strong>Rectified flow is simpler &amp; faster</strong>: Straight-line paths with velocity matching need far fewer steps than traditional DDPM.</p> </li> <li> <p><strong>Modern components matter</strong>: AdaLN, RMSNorm, Vision RoPE, and torch.compile all contribute to efficiency.</p> </li> <li> <p><strong>Oversampling works</strong>: Weighted sampling of rare color samples prevents the model from ignoring them.</p> </li> </ol> <hr> <h2 id="part-8-code--model">Part 8: Code &amp; Model</h2> <p><strong>Model weights</strong>: <a href="https://huggingface.co/ruwwww/waifu_diffusion" rel="external nofollow noopener" target="_blank">ruwwww/waifu_diffusion</a><br> <strong>File</strong>: <code class="language-plaintext highlighter-rouge">waifu_diffusion_1280_bs256.safetensors</code> (130M params)</p> <p><strong>Quick inference</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">safetensors.torch</span> <span class="kn">import</span> <span class="n">load_file</span>
<span class="kn">from</span> <span class="n">skimage</span> <span class="kn">import</span> <span class="n">color</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">JiT</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="nf">load_file</span><span class="p">(</span><span class="sh">"</span><span class="s">waifu_diffusion_1280_bs256.safetensors</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># Generate
</span><span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">step</span> <span class="o">/</span> <span class="mi">50</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">pred_x1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">top_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">left_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_x1</span> <span class="o">-</span> <span class="n">xt</span><span class="p">)</span> <span class="o">/</span> <span class="nf">max</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">step</span><span class="o">/</span><span class="mi">50</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">xt</span> <span class="o">+</span> <span class="n">v</span> <span class="o">/</span> <span class="mi">50</span>

<span class="c1"># Convert CIELAB → RGB
</span><span class="n">lab</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">pred_x1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="n">lab</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">50</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">lab</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">lab</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">rgb</span> <span class="o">=</span> <span class="n">color</span><span class="p">.</span><span class="nf">lab2rgb</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">L</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div> <p>Full training code is in the repository.</p> <hr> <h2 id="references">References</h2> <ul> <li> <strong>Rectified Flow</strong>: <a href="https://arxiv.org/abs/2210.02747" rel="external nofollow noopener" target="_blank">Flow Matching for Generative Modeling</a> </li> <li> <strong>DiT</strong>: <a href="https://arxiv.org/abs/2212.09748" rel="external nofollow noopener" target="_blank">Scalable Diffusion Models with Transformers</a> </li> <li> <strong>Vision RoPE</strong>: <a href="https://arxiv.org/abs/2104.09864" rel="external nofollow noopener" target="_blank">RoFormer: Enhanced Transformer with Rotary Position Embedding</a> </li> </ul> <hr> <table> <tbody> <tr> <td><em>March 2026</em></td> <td>Model: <a href="https://huggingface.co/ruwwww/waifu_diffusion" rel="external nofollow noopener" target="_blank">ruwwww/waifu_diffusion</a> </td> </tr> </tbody> </table> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script defer src="/al-folio/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Abdurrahman Izzuddin Al Faruq. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/al-folio/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/al-folio/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/al-folio/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/al-folio/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/al-folio/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/al-folio/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/al-folio/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/al-folio/assets/js/search-data.js"></script> <script src="/al-folio/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>